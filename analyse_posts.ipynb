{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import matplotlib\n",
    "from tools import *\n",
    "import pandas as pd\n",
    "from os.path import join as path_join\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm.notebook import tqdm as tqdm\n",
    "import matplotlib\n",
    "import os\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_export(name):\n",
    "    global STUDY_PARAM\n",
    "    plt.savefig(f'./data/figs/{name}.pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMP_DIR = f\"./data/tmp_pickles/\"\n",
    "DATA_DIR = f\"./data/\"\n",
    "\n",
    "def pickelize_to_TEMP_DIR(saving_obj, file_name):\n",
    "    pickelize(saving_obj, path_join(TEMP_DIR, file_name))\n",
    "\n",
    "\n",
    "def un_pickelize_from_TEMP_DIR(file_name):\n",
    "    return un_pickelize(path_join(TEMP_DIR, file_name))\n",
    "\n",
    "\n",
    "BUCKETS_NO = 4\n",
    "MIN_INTERACTIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_plt():\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams.update(matplotlib.rcParamsDefault)\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": \"DejaVu Sans\",  # use serif/main font for text elements\n",
    "        \"font.size\": \"18\",\n",
    "        \"figure.figsize\": (20, 10),\n",
    "        # \"font.weight\": \"bold\",\n",
    "    })\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = pd.read_csv(path_join(DATA_DIR, 'articles.csv'))\n",
    "all_included_urls = list(articles_df['canonical_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_join(TEMP_DIR, \"all_posts_info_0.pkl\")):\n",
    "    all_posts_info = {}\n",
    "    for url in tqdm(all_included_urls):\n",
    "        all_posts_info[url] = get_all_posts_of_article(url)\n",
    "    pickelize_to_TEMP_DIR(all_posts_info, \"all_posts_info_0.pkl\")\n",
    "else:\n",
    "    all_posts_info = un_pickelize_from_TEMP_DIR(\"all_posts_info_0.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_join(TEMP_DIR, \"all_posts_info_1.pkl\")):\n",
    "    all_posts_info = un_pickelize_from_TEMP_DIR(\"all_posts_info_1.pkl\")\n",
    "else:\n",
    "    for url in tqdm(all_included_urls):\n",
    "        update_posts_time_series_to_seconds(all_posts_info[url])\n",
    "    pickelize_to_TEMP_DIR(all_posts_info, \"all_posts_info_1.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = sum(1 for url in all_posts_info if len(all_posts_info[url]) > 0)\n",
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if os.path.exists(path_join(TEMP_DIR, \"all_posts_info_2.pkl\")):\n",
    "    all_posts_info = un_pickelize_from_TEMP_DIR(\"all_posts_info_2.pkl\")\n",
    "else:\n",
    "    stats = {\n",
    "        \"total_number_of_posts\": 0,\n",
    "        \"dropped_posts_counter_not_having_history\": 0,\n",
    "        \"dropped_posts_counter_not_having_enough_interaction\": 0,\n",
    "        \"sum_interaction_included\": 0,\n",
    "        \"sum_not_included\": 0,\n",
    "    }\n",
    "\n",
    "    def process_article(posts):\n",
    "        new_posts = []\n",
    "        for post in posts:\n",
    "            stats[\"total_number_of_posts\"] += 1\n",
    "            post_history = post[\"history\"]\n",
    "\n",
    "            if len(post_history[\"all_interactions\"]) > 0:\n",
    "                if post_history[\"all_interactions\"][-1] >= 10:\n",
    "                    stats[\"sum_interaction_included\"] += post_history[\"all_interactions\"][-1]\n",
    "                    new_posts.append(post)\n",
    "                else:\n",
    "                    stats[\"sum_not_included\"] += post_history[\"all_interactions\"][-1]\n",
    "                    stats[\"dropped_posts_counter_not_having_enough_interaction\"] += 1\n",
    "            else:\n",
    "                stats[\"dropped_posts_counter_not_having_history\"] += 1\n",
    "\n",
    "        return new_posts\n",
    "\n",
    "    all_posts_info = {article_index: process_article(posts) for article_index, posts in all_posts_info.items()}\n",
    "\n",
    "    print(f\"total_number_of_posts: {stats['total_number_of_posts']}\")\n",
    "    print(f\"dropped_posts_counter_not_having_history: {stats['dropped_posts_counter_not_having_history']}\")\n",
    "    print(f\"dropped_posts_counter_not_having_enough_interaction: {stats['dropped_posts_counter_not_having_enough_interaction']}\")\n",
    "    print(f\"remaining number of posts: {stats['total_number_of_posts'] - stats['dropped_posts_counter_not_having_history'] - stats['dropped_posts_counter_not_having_enough_interaction']}\")\n",
    "    print(f\"sum_interaction_included: {stats['sum_interaction_included']}\")\n",
    "    print(f\"sum_not_included: {stats['sum_not_included']}\")\n",
    "\n",
    "    pickelize_to_TEMP_DIR(all_posts_info, \"all_posts_info_2.pkl\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_number_of_posts: 262144\n",
    "dropped_posts_counter_not_having_history: 32444\n",
    "dropped_posts_counter_not_having_enough_interaction: 124679\n",
    "remaining number of posts: 105021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(path_join(TEMP_DIR, \"all_posts_info_3.pkl\")):\n",
    "    all_posts_info = un_pickelize_from_TEMP_DIR(\"all_posts_info_3.pkl\")\n",
    "else:\n",
    "    new_all_posts_info = {}\n",
    "    for article_index in all_posts_info.keys():\n",
    "        if len(all_posts_info[article_index]) > 0:\n",
    "            new_all_posts_info[article_index] = all_posts_info[article_index]\n",
    "    print(len(all_posts_info), len(new_all_posts_info))\n",
    "    pickelize_to_TEMP_DIR(new_all_posts_info, \"all_posts_info_3.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_all_posts_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_included_urls = list(all_posts_info.keys())\n",
    "len(all_included_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for url in all_posts_info.keys():\n",
    "    t = all_posts_info[url]\n",
    "    for post in t:\n",
    "        count += max(post[\"history\"][\"all_interactions\"])\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_posts_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import operator\n",
    "\n",
    "\n",
    "def seconds_converter(time):\n",
    "    day = time // (24 * 3600)\n",
    "    time = time % (24 * 3600)\n",
    "    hour = time // 3600\n",
    "    time %= 3600\n",
    "    minutes = time // 60\n",
    "    time %= 60\n",
    "    seconds = time\n",
    "    return (\"{}d:{}h:{}m:{}s\".format(day, hour, minutes, seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_posts = set()\n",
    "excluded_posts = un_pickelize_from_TEMP_DIR(\"excluded_posts\")##### comment in the first run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_date_in_seconds = 0\n",
    "for index in all_posts_info.keys():\n",
    "    for post in all_posts_info[index]:\n",
    "        post_history = post[\"history\"]\n",
    "        if max(post_history[\"date\"]) > max_date_in_seconds:\n",
    "            max_date_in_seconds = max(post_history[\"date\"])\n",
    "print(seconds_converter(max_date_in_seconds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seconds_converter(max_date_in_seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def get_cumulative_time_series_exponential_version(seed, factor):\n",
    "    global excluded_posts\n",
    "    bucket_centers = []\n",
    "    bc = seed\n",
    "    while bc <= max_date_in_seconds:\n",
    "        bucket_centers.append(bc)\n",
    "        bc *= factor\n",
    "    bucket_centers.append(max_date_in_seconds)\n",
    "\n",
    "    yhat = []\n",
    "    xhat = []\n",
    "\n",
    "    for i in range(len(bucket_centers)):\n",
    "        all_max_values = [\n",
    "            np.max(post[\"history\"][\"all_interactions\"][post[\"history\"][\"date\"] <= bucket_centers[i]]) * 100 / np.max(post[\"history\"][\"all_interactions\"])\n",
    "            for url in all_posts_info.keys()\n",
    "            for post in all_posts_info[url]\n",
    "            if post[\"platformId\"] not in excluded_posts\n",
    "            and len(post[\"history\"][\"all_interactions\"][post[\"history\"][\"date\"] <= bucket_centers[i]]) > 0\n",
    "        ]\n",
    "\n",
    "        xhat.append(bucket_centers[i])\n",
    "        yhat.append(np.mean(all_max_values))\n",
    "        print(xhat[-1], yhat[-1])\n",
    "\n",
    "    return xhat, yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new, y_new = get_cumulative_time_series_exponential_version(900, 1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = get_raw_plt()\n",
    "plt.plot(x_new, y_new)\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "y_points = []\n",
    "for i in range(0, 99, 100//BUCKETS_NO):\n",
    "    y_points.append(i)\n",
    "y_points.append(99)\n",
    "\n",
    "y_points_thresh = []\n",
    "x_points_thresh = []\n",
    "ii = []\n",
    "for v in y_points[1:]:\n",
    "    for i in range(len(y_new)):\n",
    "        if y_new[i] >= v:\n",
    "            ii.append(i)\n",
    "            m = (y_new[i]-y_new[i-1])/(x_new[i]-x_new[i-1])\n",
    "            x_points_thresh.append(int(x_new[i-1] + (v-y_new[i-1])/m))\n",
    "            y_points_thresh.append(v)\n",
    "            break\n",
    "for i in range(len(y_points_thresh)):\n",
    "    plt.vlines(x=x_points_thresh[i], ymin=0,\n",
    "               ymax=y_points_thresh[i], colors='r', linestyles='dashed')\n",
    "    plt.hlines(y=y_points_thresh[i], xmin=0,\n",
    "               xmax=x_points_thresh[i], colors='r', linestyles='dashed')\n",
    "\n",
    "x_points_thresh.append(x_new[-1])\n",
    "\n",
    "x_labels = [seconds_converter(x_points_thresh[i])\n",
    "            for i in range(len(x_points_thresh))]\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.ylim(0, max(y_new))\n",
    "plt.xlim(0, x_points_thresh[-1])\n",
    "plt.xticks(ticks=x_points_thresh, labels=x_labels, rotation=45, ha=\"right\")\n",
    "plt.yticks([int(temp) for temp in y_points_thresh], [int(temp)\n",
    "           for temp in y_points_thresh])\n",
    "# plt.savefig(\"InteractionsFraction.pdf\",bbox_inches='tight')\n",
    "\n",
    "plt.xlabel(\"Time after posting (log scale)\")\n",
    "plt.ylabel(\"Interactions fraction (percentage)\", labelpad=10)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_plt1():\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams.update(matplotlib.rcParamsDefault)\n",
    "    plt.rcParams.update({\n",
    "        # \"font.family\": \"Times\",  # use serif/main font for text elements\n",
    "        \"font.size\": \"30\",\n",
    "        \"figure.figsize\": (20, 10),\n",
    "        # \"font.weight\": \"bold\",\n",
    "\n",
    "    })\n",
    "    return plt\n",
    "\n",
    "\n",
    "plt = get_raw_plt1()\n",
    "plt.plot(x_new, y_new)\n",
    "plt.xscale(\"log\")\n",
    "\n",
    "y_points = []\n",
    "for i in range(0, 99, 100//BUCKETS_NO):\n",
    "    y_points.append(i)\n",
    "y_points.append(99)\n",
    "\n",
    "y_points_thresh = []\n",
    "x_points_thresh = []\n",
    "ii = []\n",
    "for v in y_points[1:]:\n",
    "    for i in range(len(y_new)):\n",
    "        if y_new[i] >= v:\n",
    "            ii.append(i)\n",
    "            m = (y_new[i]-y_new[i-1])/(x_new[i]-x_new[i-1])\n",
    "            x_points_thresh.append(int(x_new[i-1] + (v-y_new[i-1])/m))\n",
    "            y_points_thresh.append(v)\n",
    "            break\n",
    "for i in range(len(y_points_thresh)):\n",
    "    plt.vlines(x=x_points_thresh[i], ymin=0,\n",
    "               ymax=y_points_thresh[i], colors='r', linestyles='dashed')\n",
    "    plt.hlines(y=y_points_thresh[i], xmin=0,\n",
    "               xmax=x_points_thresh[i], colors='r', linestyles='dashed')\n",
    "\n",
    "x_points_thresh.append(x_new[-1])\n",
    "\n",
    "x_labels = ['1 h, 17 min',\n",
    "            '5 h, 16 min',\n",
    "            '17 h, 28 min',\n",
    "            '5 d, 4 h, 45 min',\n",
    "            '22 d']\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.ylim(0, max(y_new))\n",
    "plt.xlim(0, x_points_thresh[-1])\n",
    "plt.xticks(ticks=x_points_thresh, labels=x_labels, rotation=30, ha=\"right\")\n",
    "plt.yticks([int(temp) for temp in y_points_thresh], [int(temp)\n",
    "           for temp in y_points_thresh])\n",
    "\n",
    "\n",
    "plt.xlabel(\"Time after posting (log scale)\")\n",
    "plt.ylabel(\"Interactions fraction (percentage)\", labelpad=10)\n",
    "plt_export(\"InteractionsFraction\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_points_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_thresholds = x_points_thresh[:-2]\n",
    "buckets_thresholds.append(max_date_in_seconds)\n",
    "buckets_thresholds.insert(0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in buckets_thresholds:\n",
    "    print(seconds_converter(elem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_bucket(time):\n",
    "    for i in range(len(buckets_thresholds)-1):\n",
    "        if time >= buckets_thresholds[i] and time < buckets_thresholds[i+1]:\n",
    "            return i\n",
    "    raise Exception(\"time is out of range\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_excluded_posts = excluded_posts\n",
    "\n",
    "def should_exclude_post(post, buckets_thresholds):\n",
    "    post_history = post[\"history\"]\n",
    "    for i in range(len(buckets_thresholds) - 1):\n",
    "        thres1 = buckets_thresholds[i]\n",
    "        thres2 = buckets_thresholds[i + 1]\n",
    "        t = (post_history[\"date\"] < thres2) & (post_history[\"date\"] >= thres1)\n",
    "        post_values = post_history[\"all_interactions\"][t]\n",
    "        if post_values.shape[0] == 0:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "excluded_posts = {\n",
    "    post[\"platformId\"]\n",
    "    for url in all_posts_info.keys()\n",
    "    for post in all_posts_info[url]\n",
    "    if should_exclude_post(post, buckets_thresholds)\n",
    "}\n",
    "\n",
    "if previous_excluded_posts == excluded_posts:\n",
    "    print(\"excluding posts has converged\")\n",
    "else:\n",
    "    print(\"excluding posts has not converged\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickelize_to_TEMP_DIR(excluded_posts, \"excluded_posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(excluded_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the excluded posts has not converged run the above procedure again with uncommented excluded_posts = un_pickelize_from_TEMP_DIR(\"excluded_posts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_posts_info.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df = articles_df[articles_df[\"canonical_url\"].isin(all_posts_info.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_df[\"source\"].unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_included_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reliability_thresholds = [0+i*16 for i in range(5)]\n",
    "bias_thresholds = [-42, -18, -6, 6, 18, 42]\n",
    "reliability_thresholds, bias_thresholds, buckets_thresholds\n",
    "\n",
    "bias_classes = [\n",
    "    \"Far left\",\n",
    "    \"Left\",\n",
    "    \"Balanced\",\n",
    "    \"Right\",\n",
    "    \"Far right\",\n",
    "    \"All\"]\n",
    "\n",
    "\n",
    "reliability_classes = [\n",
    "    \"Most unreliable\",\n",
    "    \"Unreliable\",\n",
    "    \"Reliable\",\n",
    "    \"Most reliable\",\n",
    "    \"All\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "for j in tqdm(range(len(bias_thresholds)-1)):\n",
    "    if j == len(bias_thresholds)-1:\n",
    "        bias_indexes = articles_df.index\n",
    "    elif j == 0 or j == 4:\n",
    "        bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "            articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "    elif j == 1 :\n",
    "        bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "            articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "    elif j ==2:\n",
    "        bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "            articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "    elif j == 3:\n",
    "        bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "            articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "    else:\n",
    "        print(\"sdf\")\n",
    "    count += len(bias_indexes)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0 \n",
    "\n",
    "for k in range(len(reliability_thresholds)-1):\n",
    "    if k != (len(reliability_thresholds)-1):\n",
    "        if k == 3:\n",
    "            reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                articles_df[\"reliability\"] <= reliability_thresholds[k+1])].index\n",
    "        else:\n",
    "            reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                articles_df[\"reliability\"] < reliability_thresholds[k+1])].index\n",
    "    else:\n",
    "        reliability_indexes = articles_df.index\n",
    "    count += len(reliability_indexes)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(all_posts_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_raw_plt():\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams.update(matplotlib.rcParamsDefault)\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": 'sans-serif',  # use serif/main font for text elements\n",
    "        \"font.size\": \"25\",\n",
    "        # \"font.weight\": \"bold\",\n",
    "\n",
    "    })\n",
    "    return plt\n",
    "\n",
    "\n",
    "counts = np.zeros((6, 5))\n",
    "\n",
    "target_goals = [\"normalized_number_of_posts\", \"number_of_posts\", \"number_of_articles\", \"number_of_reactions\",\n",
    "                \"normalized_number_of_reactions\", \"likeCount\", \"shareCount\", \"commentCount\", \"deep_all\"]\n",
    "target_goals = [\"number_of_articles\", \"number_of_posts\", \"normalized_number_of_posts\", \"number_of_reactions\",\n",
    "                \"normalized_number_of_reactions\"]\n",
    "\n",
    "for target_goal in target_goals:\n",
    "    for j in tqdm(range(len(bias_thresholds))):\n",
    "        if j == len(bias_thresholds)-1:\n",
    "            bias_indexes = articles_df.index\n",
    "        elif j == 0 or j == 4:\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "        elif j == 1 :\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "        elif j ==2:\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "        elif j == 3:\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "            \n",
    "        for k in range(len(reliability_thresholds)):\n",
    "            if k != (len(reliability_thresholds)-1):\n",
    "                if k == 3:\n",
    "                    reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                        articles_df[\"reliability\"] <= reliability_thresholds[k+1])].index\n",
    "                else:\n",
    "                    reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                        articles_df[\"reliability\"] < reliability_thresholds[k+1])].index\n",
    "            else:\n",
    "                reliability_indexes = articles_df.index\n",
    "             \n",
    "            if len (reliability_indexes) == 0 :\n",
    "                print(\"reliability_indexes is empty\")\n",
    "                \n",
    "            indexes = list(set(bias_indexes).intersection(\n",
    "                reliability_indexes))\n",
    "            index_urls = articles_df.loc[indexes][\"canonical_url\"].values\n",
    "            if target_goal == \"number_of_articles\":\n",
    "                counts[j][k] = len(indexes)\n",
    "            elif target_goal == \"number_of_posts\":\n",
    "                cntt = 0\n",
    "                for index in index_urls:\n",
    "                    cntt += len(all_posts_info[index])\n",
    "                counts[j][k] = cntt\n",
    "            elif target_goal == \"normalized_number_of_posts\":\n",
    "                cntt = 0\n",
    "                for index in index_urls:\n",
    "                    cntt += len(all_posts_info[index])\n",
    "                counts[j][k] = cntt/len(indexes)\n",
    "            elif target_goal == \"number_of_reactions\":\n",
    "                cntt = 0\n",
    "                for index in index_urls:\n",
    "                    for post in all_posts_info[index]:\n",
    "                        cntt += max(post[\"history\"][\"all_interactions\"])\n",
    "                counts[j][k] = cntt\n",
    "            elif target_goal == \"normalized_number_of_reactions\":\n",
    "                cntt = 0\n",
    "                cntt2 = 0\n",
    "                for index in index_urls:\n",
    "                    for post in all_posts_info[index]:\n",
    "                        cntt += max(post[\"history\"][\"all_interactions\"])\n",
    "                        cntt2 += 1\n",
    "                counts[j][k] = cntt/cntt2\n",
    "            elif target_goal == \"likeCount\":\n",
    "                cntt = 0\n",
    "                cntt2 = 0\n",
    "                for index in index_urls:\n",
    "                    for post in all_posts_info[index]:\n",
    "                        cntt += max(post[\"history\"][\"likeCount\"])\n",
    "                        cntt2 += 1\n",
    "                counts[j][k] = cntt/cntt2\n",
    "            elif target_goal == \"commentCount\":\n",
    "                cntt = 0\n",
    "                cntt2 = 0\n",
    "                for index in index_urls:\n",
    "                    for post in all_posts_info[index]:\n",
    "                        cntt += max(post[\"history\"][\"commentCount\"])\n",
    "                        cntt2 += 1\n",
    "                counts[j][k] = cntt/cntt2\n",
    "            elif target_goal == \"shareCount\":\n",
    "                cntt = 0\n",
    "                cntt2 = 0\n",
    "                for index in index_urls:\n",
    "                    for post in all_posts_info[index]:\n",
    "                        cntt += max(post[\"history\"][\"shareCount\"])\n",
    "                        cntt2 += 1\n",
    "                counts[j][k] = cntt/cntt2\n",
    "           \n",
    "    def human_format(num):\n",
    "        num = float('{:.3g}'.format(num))\n",
    "        magnitude = 0\n",
    "        while abs(num) >= 1000:\n",
    "            magnitude += 1\n",
    "            num /= 1000.0\n",
    "        return '{}{}'.format('{:f}'.format(num).rstrip('0').rstrip('.'), ['', 'K', 'M', 'B', 'T'][magnitude])\n",
    "    \n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    mycmap = cm.get_cmap('coolwarm', 1000)  # Define the colormap for the heatmap.\n",
    "    norm = matplotlib.colors.LogNorm(vmin=counts.min(), vmax=counts.max())  # Set logarithmic normalization for the colormap.\n",
    "\n",
    "    im = ax.imshow(counts, cmap=mycmap, norm=norm)  # Display the heatmap using the defined colormap and normalization.\n",
    "\n",
    "    # Configure axis ticks and labels.\n",
    "    ax.set_xticks(np.arange(len(reliability_classes)))\n",
    "    ax.set_yticks(np.arange(len(bias_classes)))\n",
    "    ax.set_xticklabels(reliability_classes, fontsize=35)\n",
    "    ax.set_yticklabels(bias_classes, fontsize=35)\n",
    "    ax.invert_yaxis()\n",
    "\n",
    "    # Rotate x-axis tick labels.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "\n",
    "\n",
    "    # Add the count values as text on each heatmap cell.\n",
    "    for i in range(len(bias_classes)):\n",
    "        for j in range(len(reliability_classes)):\n",
    "\n",
    "            if target_goal == \"number_of_reactions\":\n",
    "                text = ax.text(j, i, \"{}\".format(human_format(int(counts[i, j]))),\n",
    "                               ha=\"center\", va=\"center\", color=\"0.0\", fontsize=24, fontweight=\"bold\")\n",
    "            else:\n",
    "                if target_goal != \"deep_all\":\n",
    "                    text = ax.text(j, i, int(counts[i, j]),\n",
    "                                   ha=\"center\", va=\"center\", color=\"0.0\", fontsize=24, fontweight=\"bold\")\n",
    "                else:\n",
    "                    text = ax.text(j, i, \"{:.2f}\".format(counts[i, j]),\n",
    "                                   ha=\"center\", va=\"center\", color=\"0.0\", fontsize=24, fontweight=\"bold\")\n",
    "\n",
    "\n",
    "    # Add a colorbar to the plot.\n",
    "    cbar = fig.colorbar(im, pad=0.01)\n",
    "    for t in cbar.ax.get_yticklabels():\n",
    "        t.set_fontsize(24)\n",
    "\n",
    "    # Save and display the plot.\n",
    "    plt_export(target_goal)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in articles_df.groupby('source').size().sort_values(ascending=False).iteritems():\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moniker_names = [\"The New York Times\", 'Fox News (website)', 'CNN (website)', 'New York Post', 'NPR', 'Reuters']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for moniker in moniker_names:\n",
    "    moniker_df = articles_df[articles_df['source'] == moniker]\n",
    "    print(moniker)\n",
    "    \n",
    "    cnt = 0\n",
    "    for url in moniker_df[\"canonical_url\"].values:\n",
    "        cnt += len(all_posts_info[url])\n",
    "    print(\"number of articles : \", moniker_df.shape[0])\n",
    "    print(\"number of posts : \", cnt)\n",
    "    print(\"number of reactions : \", sum([max(post[\"history\"][\"all_interactions\"])\n",
    "          for url in moniker_df[\"canonical_url\"].values  for post in all_posts_info[url]]))\n",
    "    print(\"normalized number of posts : \", cnt/moniker_df.shape[0])\n",
    "    print(\"normalized number of reactions : \", sum([max(post[\"history\"][\"all_interactions\"])\n",
    "          for url in moniker_df[\"canonical_url\"].values  for post in all_posts_info[url]])/cnt)\n",
    "    print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to get all the posts of a given bucket.\n",
    "def get_all_posts_of_bucket(article_indexes, bucket_no, interaction_type, previous_bucket_results=None):\n",
    "    post_results = []\n",
    "    post_maxes = {}\n",
    "    global all_posts_info\n",
    "    for url in article_indexes:\n",
    "        for post in all_posts_info[url]:\n",
    "            if post[\"platformId\"] not in excluded_posts:\n",
    "                post_values = []\n",
    "                post_history = post[\"history\"]\n",
    "                thres1 = buckets_thresholds[bucket_no]\n",
    "                thres2 = buckets_thresholds[bucket_no+1]\n",
    "                t = (post_history[\"date\"] < thres2) & (\n",
    "                    post_history[\"date\"] >= thres1)\n",
    "                post_values = post_history[interaction_type][t]\n",
    "\n",
    "                if previous_bucket_results is not None:\n",
    "                    previous_bucket_max = previous_bucket_results[post[\"platformId\"]]\n",
    "                else:\n",
    "                    previous_bucket_max = 0\n",
    "                if post_values.shape[0] == 0:\n",
    "                    print(\"no values for post \", post[\"platformId\"])\n",
    "                assert post_values.shape[0] > 0\n",
    "                post_maxes[post[\"platformId\"]] = np.max(post_values)\n",
    "                post_results.append(\n",
    "                    (post_maxes[post[\"platformId\"]]-previous_bucket_max)*100/np.max(post_history[\"all_interactions\"]))\n",
    "    return np.array(post_results), post_maxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post[\"history\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import cycler\n",
    "import seaborn as sns\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import mpl_toolkits.axes_grid1.inset_locator as mpl_il\n",
    "from scipy import stats\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import kstest\n",
    "from scipy.stats import ttest_1samp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets_threshold_text = ['0 hours',\n",
    "                          '1 hour and 17 minutes',\n",
    "                          '5 hours and 16 minutes',\n",
    "                          '17 hours and 28 minutes',\n",
    "                          '22days', ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "def get_raw_plt():\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams.update(matplotlib.rcParamsDefault)\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": 'sans-serif',  # use serif/main font for text elements\n",
    "        \"font.size\": \"25\",\n",
    "        # \"font.weight\": \"bold\",\n",
    "\n",
    "    })\n",
    "    return plt\n",
    "\n",
    "\n",
    "for interaction_type in ['all_interactions','likeCount', 'shareCount', 'commentCount','angryCount','hahaCount']:\n",
    "    print(interaction_type)\n",
    "    temp_results = []\n",
    "\n",
    "    bucket_results_averages = {}\n",
    "    bucket_results_averages[interaction_type] = {}\n",
    "    for i in range(len(buckets_thresholds)-1):\n",
    "        bucket_results_averages[interaction_type][i] = np.zeros(\n",
    "            (len(bias_thresholds), len(reliability_thresholds)))\n",
    "\n",
    "    bucket_results_arrays = {}\n",
    "    bucket_results_arrays[interaction_type] = {}\n",
    "    for i in range(len(buckets_thresholds)-1):\n",
    "        for j in range(len(bias_thresholds)):\n",
    "            for k in range(len(reliability_thresholds)):\n",
    "                bucket_results_arrays[interaction_type][(i, j, k)] = []\n",
    "\n",
    "    for j in tqdm(range(len(bias_thresholds))):\n",
    "        if j == len(bias_thresholds)-1:\n",
    "            bias_indexes = articles_df.index\n",
    "        elif j == 0 or j == 4:\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "        elif j == 1 :\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "        elif j ==2:\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "        elif j == 3:\n",
    "            bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "                articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "            \n",
    "        for k in range(len(reliability_thresholds)):\n",
    "            if k != (len(reliability_thresholds)-1):\n",
    "                if k == 3:\n",
    "                    reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                        articles_df[\"reliability\"] <= reliability_thresholds[k+1])].index\n",
    "                else:\n",
    "                    reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                        articles_df[\"reliability\"] < reliability_thresholds[k+1])].index\n",
    "            else:\n",
    "                reliability_indexes = articles_df.index\n",
    "                \n",
    "            indexes = list(set(bias_indexes).intersection(\n",
    "                reliability_indexes))\n",
    "            \n",
    "            for i in range(0, len(buckets_thresholds)-1):\n",
    "                if i > 0:\n",
    "                    previous_bucket_maxes = post_maxes\n",
    "                else:\n",
    "                    previous_bucket_maxes = None\n",
    "                urls = articles_df.loc[indexes][\"canonical_url\"].values\n",
    "                bucket_results, post_maxes = get_all_posts_of_bucket(\n",
    "                    urls, i, interaction_type, previous_bucket_maxes)\n",
    "                \n",
    "                bucket_results_arrays[interaction_type][(\n",
    "                    i, j, k)] = bucket_results\n",
    "\n",
    "    for i in range(len(buckets_thresholds)-1):\n",
    "        for j in range(len(bias_thresholds)):\n",
    "            for k in range(len(reliability_thresholds)):\n",
    "                bucket_results_averages[interaction_type][i][j, k] = np.mean(\n",
    "                    (bucket_results_arrays[interaction_type][(i, j, k)]))\n",
    "    plt = get_raw_plt()\n",
    "\n",
    "    def see(data): return np.std(data, ddof=1) / np.sqrt(np.size(data))\n",
    "    \n",
    "\n",
    "    plt = get_raw_plt()\n",
    "    bucket_results = []\n",
    "    bucket_text = []\n",
    "\n",
    "\n",
    "    for time_bucket_index in range(BUCKETS_NO):\n",
    "        text = np.empty((6, 5), dtype=np.object_)\n",
    "        result = bucket_results_averages[interaction_type][time_bucket_index]\n",
    "        for bias_index in range(len(bias_thresholds)):\n",
    "            for reliability_index in range(len(reliability_thresholds)):\n",
    "\n",
    "                text[bias_index, reliability_index] = \"\"\n",
    "                std_error = see(bucket_results_arrays[interaction_type][(\n",
    "                    time_bucket_index, bias_index, reliability_index)])\n",
    "\n",
    "\n",
    "                thresh_old = 4.0\n",
    "                if bucket_results_averages[interaction_type][time_bucket_index][bias_index][reliability_index] != 0:\n",
    "                    if std_error*100/bucket_results_averages[interaction_type][time_bucket_index][bias_index][reliability_index] < thresh_old:\n",
    "                        text[bias_index, reliability_index] += \"\\u2605\"\n",
    "                if time_bucket_index > 0:\n",
    "                    a = bucket_results_arrays[interaction_type][(\n",
    "                        time_bucket_index, bias_index, reliability_index)]\n",
    "                    b = bucket_results_arrays[interaction_type][(\n",
    "                        time_bucket_index-1, bias_index, reliability_index)]\n",
    "                    if stats.ttest_rel(a, b)[1] < 0.05:\n",
    "                        if bucket_results_averages[interaction_type][time_bucket_index][bias_index, reliability_index] > bucket_results_averages[interaction_type][time_bucket_index-1][bias_index, reliability_index]:\n",
    "                            text[bias_index, reliability_index] += \"\\u25B2\"\n",
    "                        else:\n",
    "                            text[bias_index, reliability_index] += \"\\u25BC\"\n",
    "\n",
    "\n",
    "        bucket_results.append(result)\n",
    "        bucket_text.append(text)\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (20, 10.3)\n",
    "\n",
    "    fig = plt.figure(constrained_layout=False)\n",
    "\n",
    "    gs = GridSpec(1, BUCKETS_NO, figure=fig)\n",
    "\n",
    "    ax_first_row = fig.add_subplot(gs[0, :])\n",
    "\n",
    "    ax_first_row.arrow(0, .67, 5, 0, head_width=0.035,\n",
    "                    width=0.005, facecolor='darkgreen', color=None)\n",
    "    ax_first_row.text(2.5, 0.71, \"Time\", ha=\"center\",\n",
    "                    va=\"center\",  size=24, color=\"darkgreen\")\n",
    "\n",
    "    x = [1.07, 2.52, 3.97]\n",
    "    l = [-0.055, -0.045, -0.02]\n",
    "    for i in range(len(x)):\n",
    "        ax_first_row.arrow(x[i], .67, 0, l[i], facecolor='darkgreen',\n",
    "                        color=None, width=0.005, head_width=0.03)\n",
    "    '''\n",
    "\n",
    "    ax_first_row.scatter([0],[-2])\n",
    "    '''\n",
    "    # ax_first_row.scatter([0,1],[1,1])\n",
    "    ax_first_row.scatter([0, 1], [0, 0], color=\"white\")\n",
    "    ax_first_row.scatter([0, 1], [0, 0], color=\"white\")\n",
    "\n",
    "    for spine in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "        _ = ax_first_row.spines[spine].set_visible(False)\n",
    "    _ = ax_first_row.set_xticks([])\n",
    "    _ = ax_first_row.set_yticks([])\n",
    "\n",
    "    axes = []\n",
    "\n",
    "    for i in range(BUCKETS_NO):\n",
    "        axes.append(fig.add_subplot(gs[0, i]))\n",
    "\n",
    "    vim = 100\n",
    "    vax = 0\n",
    "    for t in range(BUCKETS_NO):\n",
    "        if np.min(bucket_results[t]) < vim:\n",
    "            vim = np.min(bucket_results[t])\n",
    "        if np.max(bucket_results[t]) > vax:\n",
    "            vax = np.max(bucket_results[t])\n",
    "    #if vim < 1:\n",
    "    #    vim = 1\n",
    "\n",
    "    cs = []\n",
    "    cs2 = []\n",
    "    if interaction_type == \"all_interactions\":\n",
    "        cs = [17, 20, 28.25, 29.5, 32]\n",
    "        for i in np.arange(22.25, 28.25, 0.5):\n",
    "            cs.append(i)\n",
    "\n",
    "        cs.append(25.5)\n",
    "        cs.sort()\n",
    "\n",
    "        cs.sort()\n",
    "    elif interaction_type == \"likeCount\":\n",
    "        vax1 = 18\n",
    "        vim1 = 5\n",
    "        for i in range(vim1, vax1, 1):\n",
    "            cs.append(i)\n",
    "        for i in np.arange(7, 12.5, .5):\n",
    "            cs.append(i)\n",
    "\n",
    "        cs.append(9.25)\n",
    "        cs.append(9.75)\n",
    "        cs = sorted(set(cs))\n",
    "    elif interaction_type == \"shareCount\":\n",
    "        vax1 = 10\n",
    "        vim1 = 3\n",
    "        for i in range(vim1, vax1, 1):\n",
    "            cs.append(i)\n",
    "        for i in np.arange(2, 5.5, .5):\n",
    "            cs.append(i)\n",
    "        cs.append(4.25)\n",
    "        cs = sorted(set(cs))\n",
    "    elif interaction_type == \"commentCount\":\n",
    "        vax1 = 7\n",
    "        vim1 = 2\n",
    "        for i in range(vim1, vax1, 1):\n",
    "            cs.append(i)\n",
    "        for i in np.arange(3, 6.5, .5):\n",
    "            cs.append(i)\n",
    "        cs.append(4.25)\n",
    "        cs = sorted(set(cs))\n",
    "    elif interaction_type == \"angryCount\":\n",
    "        cs = [1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5]\n",
    "        cs = sorted(set(cs))\n",
    "\n",
    "    elif interaction_type == \"hahaCount\":\n",
    "        cs = [0.05, 1, 1.5, 2, 2.5 ]\n",
    "        cs = sorted(set(cs))\n",
    "    elif interaction_type == \"loveCount\":\n",
    "        cs = [0, 0.25, 0.5, .75, 1, 1.5, 2, 2.5]\n",
    "        cs = sorted(set(cs))\n",
    "\n",
    "    cs.append(vax)\n",
    "    cs.insert(0, vim)\n",
    "    for elem in cs:\n",
    "        if elem > vax or elem < vim:\n",
    "            print(elem)\n",
    "    mycmap = cm.get_cmap('coolwarm', 1000)\n",
    "\n",
    "    bounds = np.array(cs)\n",
    "    norm = matplotlib.colors.BoundaryNorm(\n",
    "        boundaries=bounds, ncolors=1000, extend='both')\n",
    "\n",
    "    for time_step in range(len(bucket_results)):\n",
    "        ax = axes[time_step]\n",
    "        im = ax.imshow(bucket_results[time_step], cmap=mycmap, norm=norm)\n",
    "\n",
    "        ax.set_xticks(np.arange(len(reliability_classes)))\n",
    "        ax.set_xticklabels(reliability_classes)\n",
    "        if time_step == 0:\n",
    "            ax.set_yticks(np.arange(len(bias_classes)))\n",
    "            ax.set_yticklabels(bias_classes)\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "        ax.invert_yaxis()\n",
    "\n",
    "\n",
    "\n",
    "        if time_step != 0:\n",
    "            ax.text(-0.1, .5, buckets_threshold_text[time_step], bbox=dict(facecolor='green', alpha=0.5),\n",
    "                    horizontalalignment='center', verticalalignment='center', rotation=\"vertical\", transform=ax.transAxes, size=22)\n",
    "\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                 rotation_mode=\"anchor\")\n",
    "        for i in range(len(bias_classes)):\n",
    "            for j in range(len(reliability_classes)):\n",
    "                text = ax.text(j, i, bucket_text[time_step][i][j],\n",
    "                               ha=\"center\", va=\"center\", color=\"0.0\", fontsize=16)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    cax = fig.add_axes([axes[-1].get_position().x1+0.01,\n",
    "                       ax.get_position().y0-.4, 0.02, axes[-1].get_position().height+.3])\n",
    "    cbar = fig.colorbar(im, cax=cax)\n",
    "\n",
    "    for t in cbar.ax.get_yticklabels():\n",
    "        t.set_fontsize(20)\n",
    "    plt.subplots_adjust(wspace=.2, hspace=0.8, top=.62)\n",
    "\n",
    "\n",
    "    plt_export(interaction_type)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moniker_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "def get_raw_plt():\n",
    "    import matplotlib.pyplot as plt\n",
    "    plt.rcParams.update(matplotlib.rcParamsDefault)\n",
    "    plt.rcParams.update({\n",
    "        \"font.family\": 'sans-serif',  # use serif/main font for text elements\n",
    "        \"font.size\": \"25\",\n",
    "        # \"font.weight\": \"bold\",\n",
    "\n",
    "    })\n",
    "    return plt\n",
    "\n",
    "for moniker in moniker_names:\n",
    "    print(moniker)\n",
    "    moniker_indexes = articles_df[articles_df['source'] == moniker].index\n",
    "    for interaction_type in ['all_interactions']:\n",
    "        print(interaction_type)\n",
    "        temp_results = []\n",
    "\n",
    "        bucket_results_averages = {}\n",
    "        bucket_results_averages[interaction_type] = {}\n",
    "        for i in range(len(buckets_thresholds)-1):\n",
    "            bucket_results_averages[interaction_type][i] = np.zeros(\n",
    "                (len(bias_thresholds), len(reliability_thresholds)))\n",
    "\n",
    "        bucket_results_arrays = {}\n",
    "        bucket_results_arrays[interaction_type] = {}\n",
    "        for i in range(len(buckets_thresholds)-1):\n",
    "            for j in range(len(bias_thresholds)):\n",
    "                for k in range(len(reliability_thresholds)):\n",
    "                    bucket_results_arrays[interaction_type][(i, j, k)] = []\n",
    "\n",
    "        for j in tqdm(range(len(bias_thresholds))):\n",
    "            if j == len(bias_thresholds)-1:\n",
    "                bias_indexes = articles_df.index\n",
    "            elif j == 0 or j == 4:\n",
    "                bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "                    articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "            elif j == 1 :\n",
    "                bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "                    articles_df[\"bias\"] <= bias_thresholds[j+1])].index\n",
    "            elif j ==2:\n",
    "                bias_indexes = articles_df[(articles_df[\"bias\"] > bias_thresholds[j]) & (\n",
    "                    articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "            elif j == 3:\n",
    "                bias_indexes = articles_df[(articles_df[\"bias\"] >= bias_thresholds[j]) & (\n",
    "                    articles_df[\"bias\"] < bias_thresholds[j+1])].index\n",
    "                \n",
    "            for k in range(len(reliability_thresholds)):\n",
    "                if k != (len(reliability_thresholds)-1):\n",
    "                    if k == 3:\n",
    "                        reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                            articles_df[\"reliability\"] <= reliability_thresholds[k+1])].index\n",
    "                    else:\n",
    "                        reliability_indexes = articles_df[(articles_df[\"reliability\"] >= reliability_thresholds[k]) & (\n",
    "                            articles_df[\"reliability\"] < reliability_thresholds[k+1])].index\n",
    "                else:\n",
    "                    reliability_indexes = articles_df.index\n",
    "                    \n",
    "                indexes = list(set(bias_indexes).intersection(\n",
    "                    reliability_indexes).intersection(moniker_indexes))\n",
    "                \n",
    "                for i in range(0, len(buckets_thresholds)-1):\n",
    "                    if i > 0:\n",
    "                        previous_bucket_maxes = post_maxes\n",
    "                    else:\n",
    "                        previous_bucket_maxes = None\n",
    "                    urls = articles_df.loc[indexes][\"canonical_url\"].values\n",
    "                    bucket_results, post_maxes = get_all_posts_of_bucket(\n",
    "                        urls, i, interaction_type, previous_bucket_maxes)\n",
    "                    \n",
    "                    bucket_results_arrays[interaction_type][(\n",
    "                        i, j, k)] = bucket_results\n",
    "\n",
    "        for i in range(len(buckets_thresholds)-1):\n",
    "            for j in range(len(bias_thresholds)):\n",
    "                for k in range(len(reliability_thresholds)):\n",
    "                    if len(bucket_results_arrays[interaction_type][(i, j, k)]) > 0:\n",
    "                        bucket_results_averages[interaction_type][i][j, k] = np.mean((bucket_results_arrays[interaction_type][(i, j, k)]))\n",
    "                    else:\n",
    "                        bucket_results_averages[interaction_type][i][j, k] = np.nan\n",
    "        plt = get_raw_plt()\n",
    "\n",
    "        def see(data):\n",
    "            if len(data) <= 1:\n",
    "                return np.nan\n",
    "            return np.std(data, ddof=1) / np.sqrt(np.size(data))\n",
    "\n",
    "        plt = get_raw_plt()\n",
    "        bucket_results = []\n",
    "        bucket_text = []\n",
    "\n",
    "\n",
    "\n",
    "        for time_bucket_index in range(BUCKETS_NO):\n",
    "            text = np.empty((6, 5), dtype=np.object_)\n",
    "            result = bucket_results_averages[interaction_type][time_bucket_index]\n",
    "            for bias_index in range(len(bias_thresholds)):\n",
    "                for reliability_index in range(len(reliability_thresholds)):\n",
    "\n",
    "                    text[bias_index, reliability_index] = \"\"\n",
    "                    std_error = see(bucket_results_arrays[interaction_type][(\n",
    "                        time_bucket_index, bias_index, reliability_index)])\n",
    "\n",
    "                    if interaction_type == 'all_interactions':\n",
    "                        thresh_old = 4.0\n",
    "                    else:\n",
    "                        thresh_old = 4.0\n",
    "                    if bucket_results_averages[interaction_type][time_bucket_index][bias_index][reliability_index] != 0:\n",
    "                        if std_error*100/bucket_results_averages[interaction_type][time_bucket_index][bias_index][reliability_index] < thresh_old:\n",
    "                            text[bias_index, reliability_index] += \"\\u2605\"\n",
    "                    if time_bucket_index > 0:\n",
    "                        a = bucket_results_arrays[interaction_type][(\n",
    "                            time_bucket_index, bias_index, reliability_index)]\n",
    "                        b = bucket_results_arrays[interaction_type][(\n",
    "                            time_bucket_index-1, bias_index, reliability_index)]\n",
    "                        if len(a) > 1 and len(b) > 1:\n",
    "                            if stats.ttest_rel(a, b)[1] < 0.05:\n",
    "                                if bucket_results_averages[interaction_type][time_bucket_index][bias_index, reliability_index] > bucket_results_averages[interaction_type][time_bucket_index-1][bias_index, reliability_index]:\n",
    "                                    text[bias_index, reliability_index] += \"\\u25B2\"\n",
    "                                else:\n",
    "                                    text[bias_index, reliability_index] += \"\\u25BC\"\n",
    "\n",
    "\n",
    "            bucket_results.append(result)\n",
    "            bucket_text.append(text)\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (20, 10.3)\n",
    "\n",
    "        fig = plt.figure(constrained_layout=False)\n",
    "\n",
    "        gs = GridSpec(1, BUCKETS_NO, figure=fig)\n",
    "\n",
    "        ax_first_row = fig.add_subplot(gs[0, :])\n",
    "\n",
    "        ax_first_row.arrow(0, .67, 5, 0, head_width=0.035,\n",
    "                        width=0.005, facecolor='darkgreen', color=None)\n",
    "        ax_first_row.text(2.5, 0.71, \"Time\", ha=\"center\",\n",
    "                        va=\"center\",  size=24, color=\"darkgreen\")\n",
    "\n",
    "        x = [1.07, 2.52, 3.97]\n",
    "        l = [-0.055, -0.045, -0.02]\n",
    "        for i in range(len(x)):\n",
    "            ax_first_row.arrow(x[i], .67, 0, l[i], facecolor='darkgreen',\n",
    "                            color=None, width=0.005, head_width=0.03)\n",
    "        '''\n",
    "\n",
    "        ax_first_row.scatter([0],[-2])\n",
    "        '''\n",
    "        ax_first_row.scatter([0, 1], [0, 0], color=\"white\")\n",
    "        ax_first_row.scatter([0, 1], [0, 0], color=\"white\")\n",
    "\n",
    "        for spine in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "            _ = ax_first_row.spines[spine].set_visible(False)\n",
    "        _ = ax_first_row.set_xticks([])\n",
    "        _ = ax_first_row.set_yticks([])\n",
    "\n",
    "        axes = []\n",
    "\n",
    "        for i in range(BUCKETS_NO):\n",
    "            axes.append(fig.add_subplot(gs[0, i]))\n",
    "\n",
    "        vim = 100\n",
    "        vax = 0\n",
    "        for t in range(BUCKETS_NO):\n",
    "            if np.nanmin(bucket_results[t]) < vim:\n",
    "                vim = np.nanmin(bucket_results[t])\n",
    "            if np.nanmax(bucket_results[t]) > vax:\n",
    "                vax = np.nanmax(bucket_results[t])\n",
    "\n",
    "\n",
    "        cs = []\n",
    "        cs2 = []\n",
    "        if interaction_type == \"all_interactions\":\n",
    "            cs = [ 20, 28.25, 29.5]\n",
    "            for i in np.arange(22.25, 28.25, 0.5):\n",
    "                cs.append(i)\n",
    "\n",
    "            cs.append(25.5)\n",
    "            cs.sort()\n",
    "\n",
    "            cs.sort()\n",
    "        elif interaction_type == \"likeCount\":\n",
    "            vax1 = 18\n",
    "            vim1 = 5\n",
    "            for i in range(vim1, vax1, 1):\n",
    "                cs.append(i)\n",
    "            for i in np.arange(7, 12.5, .5):\n",
    "                cs.append(i)\n",
    "\n",
    "            cs.append(9.25)\n",
    "            cs.append(9.75)\n",
    "            cs = sorted(set(cs))\n",
    "        elif interaction_type == \"shareCount\":\n",
    "            vax1 = 10\n",
    "            vim1 = 3\n",
    "            for i in range(vim1, vax1, 1):\n",
    "                cs.append(i)\n",
    "            for i in np.arange(2, 5.5, .5):\n",
    "                cs.append(i)\n",
    "            cs.append(4.25)\n",
    "            cs = sorted(set(cs))\n",
    "        elif interaction_type == \"commentCount\":\n",
    "            vax1 = 7\n",
    "            vim1 = 2\n",
    "            for i in range(vim1, vax1, 1):\n",
    "                cs.append(i)\n",
    "            for i in np.arange(3, 6.5, .5):\n",
    "                cs.append(i)\n",
    "            cs.append(4.25)\n",
    "            cs = sorted(set(cs))\n",
    "        elif interaction_type == \"angryCount\":\n",
    "            cs = [1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5]\n",
    "            cs = sorted(set(cs))\n",
    "\n",
    "        elif interaction_type == \"hahaCount\":\n",
    "            cs = [0.05, 1, 1.5, 2, 2.5 ]\n",
    "            cs = sorted(set(cs))\n",
    "        elif interaction_type == \"loveCount\":\n",
    "            cs = [0, 0.25, 0.5, .75, 1, 1.5, 2, 2.5]\n",
    "            cs = sorted(set(cs))\n",
    "\n",
    "        cs.append(vax)\n",
    "        cs.insert(0, vim)\n",
    "        for elem in cs:\n",
    "            if elem > vax or elem < vim:\n",
    "                print(elem)\n",
    "        mycmap = cm.get_cmap('coolwarm', 1000)\n",
    "\n",
    "        bounds = np.array(cs)\n",
    "        norm = matplotlib.colors.BoundaryNorm(\n",
    "            boundaries=bounds, ncolors=1000, extend='both')\n",
    "\n",
    "        for time_step in range(len(bucket_results)):\n",
    "            ax = axes[time_step]\n",
    "            im = ax.imshow(bucket_results[time_step], cmap=mycmap, norm=norm)\n",
    "\n",
    "            ax.set_xticks(np.arange(len(reliability_classes)))\n",
    "            ax.set_xticklabels(reliability_classes)\n",
    "            if time_step == 0:\n",
    "                ax.set_yticks(np.arange(len(bias_classes)))\n",
    "                ax.set_yticklabels(bias_classes)\n",
    "            else:\n",
    "                ax.set_yticks([])\n",
    "            ax.invert_yaxis()\n",
    "\n",
    "\n",
    "            if time_step != 0:\n",
    "                ax.text(-0.1, .5, buckets_threshold_text[time_step], bbox=dict(facecolor='green', alpha=0.5),\n",
    "                        horizontalalignment='center', verticalalignment='center', rotation=\"vertical\", transform=ax.transAxes, size=22)\n",
    "\n",
    "            plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "                    rotation_mode=\"anchor\")\n",
    "            for i in range(len(bias_classes)):\n",
    "                for j in range(len(reliability_classes)):\n",
    "                    text = ax.text(j, i, bucket_text[time_step][i][j],\n",
    "                                ha=\"center\", va=\"center\", color=\"0.0\", fontsize=16)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        cax = fig.add_axes([axes[-1].get_position().x1+0.01,\n",
    "                        ax.get_position().y0-.4, 0.02, axes[-1].get_position().height+.3])\n",
    "        cbar = fig.colorbar(im, cax=cax)\n",
    "\n",
    "        for t in cbar.ax.get_yticklabels():\n",
    "            t.set_fontsize(20)\n",
    "        plt.subplots_adjust(wspace=.2, hspace=0.8, top=.62)\n",
    "\n",
    "        plt_export(moniker+\"_\"+interaction_type)\n",
    "        plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aa8ca25004f0cdbefbb45b19e914c592a50d79c45d4d00948be716e0632b82c4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
